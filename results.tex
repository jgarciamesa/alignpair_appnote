\section{Results}
% prelim data comparison?

% by estimating $k_s$ and $k_a$ statistics.

% % Software comparison table
% \begin{table}[!ht]
% \centering
%     %\frame{
%     \input{figures/table-comp}%}
% 	\caption{Accuracy of COATi, PRANK, MAFFT, Clustal$\Omega$, and MACSE,
%             on 2340 simulated sequence pairs. Perfect alignments have
%             ($d_{seq}=0$), best alignments have lowest $d_{seq}$, and imperfect
%             alignments have $d_{seq}>0$ when at least one aligner found a
%             perfect alignment. Best values are highlighted in blue.}
% 	\label{table:comp}
% \end{table}

Both models in COATi were significantly (p < $2.2$x$10^{-16}$) more accurate at
inferring simulated alignments compared to other aligners according to the
one-tailed Wilcoxon signed ranked test, with an average alignment error
($d_{seq}$) value of $1$x$10^{-3}$.
In addition, COATi produced more perfect alignments ($d_{seq}=0$), less imperfect
alignments, and more accurately retrieved events of positive selection
(fst model 91.9\%, marginal model 90.8\%) (Supplementary Table 1).
Compared to COATi, MACSE (allows frameshifts) and MAFFT (using DNA) had an
average alignment error an order of magnitude larger than COATi and a lower
accuracy retrieving events of positive selection at a rate of 81.5\%
and 85.8\% respectively.
PRANK (using codons) and Clustal$\Omega$ (using amino acid translations) had
an average alignment error two orders of magnitude larger than COATi and a
87.3\% and 69.1\% accuracy retrieving events of positive selection, respectively.
The accuracy retrieving events of negative selection was similar across all five
aligners (97.7\% $\pm$ 1.67\%).

% (Table \ref{table:comp}).

